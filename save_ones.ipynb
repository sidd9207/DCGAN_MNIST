{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c021ac-20ac-4193-8c1c-5bcab55693c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3256104-19bc-4519-a247-3b05576b8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*16, 4, 1, 0),\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1),\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1),\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0af143-0df7-4e27-b4c1-aee40dc9a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Nines_generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4def5d-1ac2-477e-853f-341275d2cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn((1, 100, 1, 1)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c607457c-47fc-4dd1-ac96-51d763198414",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a56350-6d36-4e4f-9497-523695bebc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b94ac2-e7f6-4dfb-b511-338231c80b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    noise = torch.randn((50, 100, 1, 1)).to(\"cuda\")\n",
    "    output = model(noise)\n",
    "    for i in range(50):\n",
    "        image = output[i]\n",
    "        torchvision.utils.save_image(image, f\"images/nines/img{image_num}.png\")\n",
    "        image_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
